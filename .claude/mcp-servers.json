{
  "$schema": "https://raw.githubusercontent.com/anthropics/claude-code/main/schemas/mcp-servers.json",
  "mcpServers": {
    "openai-proxy": {
      "command": "pixi",
      "args": ["run", "-e", "js", "pnpm", "dlx", "@anthropic/mcp-server-openai"],
      "env": {
        "OPENAI_API_KEY": "${OPENAI_API_KEY}"
      },
      "description": "Proxy to OpenAI models (GPT-4, GPT-4o)"
    },

    "ollama": {
      "command": "pixi",
      "args": ["run", "-e", "js", "pnpm", "dlx", "@anthropic/mcp-server-ollama"],
      "env": {
        "OLLAMA_HOST": "http://localhost:11434"
      },
      "description": "Local Ollama models (llama3, mistral, codellama)"
    },

    "localai": {
      "command": "pixi",
      "args": ["run", "-e", "js", "pnpm", "dlx", "mcp-server-localai"],
      "env": {
        "LOCALAI_ENDPOINT": "http://localhost:8080/v1",
        "LOCALAI_API_KEY": "${LOCALAI_API_KEY}"
      },
      "description": "LocalAI inference server"
    },

    "litellm": {
      "command": "pixi",
      "args": ["run", "-e", "js", "pnpm", "dlx", "mcp-server-litellm"],
      "env": {
        "LITELLM_PROXY_URL": "http://localhost:4000",
        "LITELLM_API_KEY": "${LITELLM_API_KEY}"
      },
      "description": "LiteLLM proxy - unified interface to 100+ models"
    }
  }
}
