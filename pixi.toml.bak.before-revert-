[workspace]
name = "robostack"
description = "Development environment for RoboStack ROS packages"
# =============================================================================
# Channel Priority Strategy (see docs/CHANNEL-STRATEGY.md)
# =============================================================================
# Channels are listed in priority order (first = highest priority).
# - robostack-humble: ROS2 Humble packages, takes precedence for ROS compatibility
# - conda-forge: Community packages for general-purpose dependencies
#
# For CUDA environments, features add: pytorch > nvidia > conda-forge
# This ensures CUDA-linked PyTorch builds are resolved correctly.
# =============================================================================
channels = ["conda-forge"]
platforms = ["linux-64"]

[dependencies]
python = ">=3.11,<3.13"

# Used by scripts/validate-manifest.py and scripts/generate-verification.py
pyyaml = ">=6.0,<7"
jsonschema = ">=4.23,<5"

# Build tools
compilers = ">=1.11.0,<2"
cmake = ">=3.28,<4"
pkg-config = ">=0.29.2,<0.30"
make = ">=4.4.1,<5"
ninja = ">=1.13.2,<2"

# Compilation cache & fast linking
ccache = ">=4.10,<5"
sccache = ">=0.8,<1"

# Note: Rust toolchain for Agent Gateway comes from Nix, not pixi
# (cargo/rust not available on all platforms in conda-forge)

# Archive/Network tools (for tree-sitter)
tar = ">=1.34"
curl = ">=8.0"

# Node.js ecosystem (for LazyVim plugins like peek.nvim, live-preview.nvim)
nodejs = ">=22.0,<23"    # LTS "Jod" - active until Apr 2027
pnpm = ">=9.0"

# ROS specific tools are in the ros feature (see [feature.ros])

# Python modernization tools (upgrade old Python syntax to modern versions)
ruff = ">=0.8.0"
pyupgrade = ">=3.19.0"
flynt = ">=1.0.0"

# Testing & Code Quality
pytest = ">=8.0,<9"
pytest-cov = ">=4.0,<6"
mypy = ">=1.0,<2"
black = ">=24.0,<25"
isort = ">=5.0,<6"

# Scientific/ML dependencies (explicit)
numpy = ">=1.24,<3"
pandas = ">=2.0,<3"
scikit-learn = ">=1.3,<2"

# Jupyter & Interactive Development
jupyterlab = ">=4.3,<5"
ipython = ">=8.30,<9"
ipywidgets = ">=8.0,<9"
notebook = ">=7.0,<8"

# LLMOps & Evaluation
mlflow = ">=2.19,<3"
tensorboard = ">=2.0,<3"
wandb = ">=0.19,<1"

# Hugging Face ecosystem
transformers = ">=4.47,<5"
accelerate = ">=1.2,<2"
sentence-transformers = ">=3.3,<4"
datasets = ">=3.2,<4"
tokenizers = ">=0.21,<1"

# Messaging & Orchestration (BUILDKIT_STARTER_SPEC.md Layer 6)
nats-py = ">=2.9"           # NATS Python client for event bus
temporalio = ">=1.7"        # Temporal Python SDK for durable workflows

# =============================================================================
# Agent Runtime Packages (BUILDKIT_STARTER_SPEC.md Layer 7: P2-001 to P2-005)
# =============================================================================
# P2-001: agno-agi/agno - Multi-agent framework, runtime, and control plane
# Available on PyPI: https://pypi.org/project/agno/

# P2-002: ruvnet/agentic-flow - CANNOT BE INSTALLED VIA PIXI
# This is a Node.js/TypeScript package, not a Python package
# Repository: https://github.com/ruvnet/agentic-flow
# Install via npm: npm install agentic-flow@alpha

# P2-003: ruvnet/claude-flow - CANNOT BE INSTALLED VIA PIXI
# This is a Node.js/TypeScript package, not a Python package
# Repository: https://github.com/ruvnet/claude-flow
# Install via npm: npm install -g claude-flow@alpha

[pypi-dependencies]
agno = ">=0.1"
# P2-004: ruvnet/Synaptic-Mesh - CANNOT BE INSTALLED VIA PIP
# This is a Rust/TypeScript/WASM project, not a Python package
# Repository: https://github.com/ruvnet/Synaptic-Mesh
# Multi-language project with Rust core, TypeScript CLI, WebAssembly compilation
# Install via: npx synaptic-mesh init (after cloning and building)

# P2-005: ruvnet/daa - CANNOT BE INSTALLED VIA PIP
# This is a Rust SDK with Node.js bindings (NAPI), not a Python package
# Repository: https://github.com/ruvnet/daa
# Decentralized Autonomous Applications SDK written in Rust
# Use the Rust crates directly or Node.js bindings

# =============================================================================
# Additional PyPI-only dependencies
# =============================================================================

# Build tools (JavaScript/TypeScript)

# Data processing
datafusion = ">=44.0"
polars = ">=1.18"

# =============================================================================
# PyTorch ML Stack (CPU by default, CUDA via feature)
# =============================================================================
# IMPORTANT: Version coupling is REQUIRED - mismatched versions cause runtime errors!
# See docs/CONFLICTS.md for full compatibility matrix and upgrade paths.
#
# Version Coupling Table:
#   PyTorch 2.5.x requires torchvision 0.20.x, torchaudio 2.5.x
#   PyTorch 2.4.x requires torchvision 0.19.x, torchaudio 2.4.x
#   PyTorch 2.3.x requires torchvision 0.18.x, torchaudio 2.3.x
#
# To upgrade: ./scripts/upgrade-python-deps.sh pytorch <version>
# To verify: ./scripts/check-python-deps.sh

# =============================================================================
# ROS / RoboStack Feature
# =============================================================================

[feature.ros]
platforms = ["linux-64"]
channels = ["robostack-humble", "conda-forge"]

[feature.ros.dependencies]
rosdep = ">=0.26.0,<0.27"
colcon-common-extensions = ">=0.3.0,<0.4"
ros-humble-desktop = ">=0.10.0,<0.11"
catkin_tools = ">=0.8.2,<0.10"

[feature.cuda]
# CUDA-enabled PyTorch (requires NVIDIA GPU)
# Activate with: pixi run -e cuda <command>
#
# Channel Priority (see docs/CHANNEL-STRATEGY.md):
# - pytorch: Official CUDA-linked PyTorch builds (highest priority)
# - nvidia: CUDA runtime and cuDNN libraries
# - conda-forge: Supporting packages (lowest priority)
#
# WARNING: Do not combine with robostack-humble in the same solve-group
# as they have incompatible Python/CUDA expectations.
platforms = ["linux-64"]
channels = ["pytorch", "nvidia", "conda-forge"]

[feature.cuda.dependencies]
# PyTorch with CUDA from pytorch channel
# IMPORTANT: Keep versions synchronized with base [pypi-dependencies]!
# Version coupling: PyTorch 2.5.x requires torchvision 0.20.x, torchaudio 2.5.x
# See docs/CONFLICTS.md for CUDA version compatibility
pytorch-cuda = { version = "13.1.*", channel = "pytorch" }
pytorch = { version = ">=2.5,<3", channel = "pytorch" }

# =============================================================================
# AIOS Agent OS Feature
# =============================================================================
# AIOS (AI Agent Operating System) requires strict dependency pinning
# to ensure compatibility with the agent kernel and Cerebrum SDK.
#
# CRITICAL CONSTRAINTS:
#   - Python 3.10-3.11 ONLY (no 3.12+ support - APIs removed)
#   - pydantic==2.7.0 (exact version required)
#   - numpy==1.24.3 (exact version required)
#   - click==8.1.7 (exact version required)
#
# See: https://github.com/agiresearch/AIOS
#      https://github.com/agiresearch/Cerebrum
#      docs/CONFLICTS.md#aios-compatibility
#
# Activate with: pixi run -e aios <command>

[feature.aios]
# AIOS requires Python 3.10-3.11 (no 3.12+ support due to removed APIs)
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.aios.dependencies]
# Core AIOS dependencies with strict pins for compatibility
pydantic = "==2.7.0"
numpy = ">=1.26.4,<3"

# LLM & Embedding dependencies
litellm = ">=1.0"
transformers = ">=4.30"
accelerate = ">=0.20"
sentence-transformers = ">=2.0"

# Vector database (AIOS default)
chromadb = ">=0.4"

# Cache & messaging
redis-py = ">=4.5.1"

# API server
fastapi = ">=0.100"
uvicorn = ">=0.20"

# Utilities
python-dotenv = ">=1.0"
watchdog = ">=2.1.9"
rich = ">=13.0"
click = "==8.1.7"

# NLP
nltk = ">=3.8"

# Cerebrum SDK dependencies
requests = ">=2.28"
platformdirs = ">=3.0"
datasets = ">=2.14"

# Cerebrum Agent SDK (P0 - Required for AIOS agent development)
# See: https://github.com/agiresearch/Cerebrum
pyautogen = ">=0.2"         # AutoGen for multi-agent orchestration

[feature.aios.pypi-dependencies]
# Cerebrum SDK - AIOS Agent Development Kit (not in conda-forge)
# P0: Required per BUILDKIT_STARTER_SPEC.md Layer 7
aios-agent-sdk = "==0.0.3"

# =============================================================================
# AIOS + CUDA Feature
# =============================================================================
# AIOS with GPU acceleration via vLLM
# Activate with: pixi run -e aios-cuda <command>

[feature.aios-cuda]
platforms = ["linux-64"]

[feature.aios-cuda.dependencies]
# Includes all AIOS dependencies plus vLLM
pydantic = "==2.7.0"
numpy = ">=1.26.4,<3"
litellm = ">=1.0"
transformers = ">=4.30"
accelerate = ">=0.20"
sentence-transformers = ">=2.0"
chromadb = ">=0.4"
redis-py = ">=4.5.1"
fastapi = ">=0.100"
uvicorn = ">=0.20"
python-dotenv = ">=1.0"
watchdog = ">=2.1.9"
rich = ">=13.0"
click = "==8.1.7"
nltk = ">=3.8"
requests = ">=2.28"
platformdirs = ">=3.0"
datasets = ">=2.14"

# =============================================================================
# LLMOps Feature - Evaluation & Experiment Tracking
# =============================================================================
# TruLens for runtime LLM evaluation and MLflow for experiment tracking
# BUILDKIT_STARTER_SPEC.md Layer 12: LLMOps & Evaluation
# Activate with: pixi run -e llmops <command>

[feature.llmops]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.llmops.dependencies]
# TruLens - Runtime LLM evaluation
# See: https://github.com/truera/trulens

# MLflow - Experiment tracking & model registry
# See: https://github.com/mlflow/mlflow
mlflow = ">=2.10"

# OpenTelemetry for instrumentation
opentelemetry-api = ">=1.20"
opentelemetry-sdk = ">=1.20"
opentelemetry-exporter-otlp = ">=1.20"

# Evaluation dependencies
pandas = ">=2.0"
scipy = ">=1.10"
scikit-learn = ">=1.3"

# =============================================================================
# Finetuning Feature - LLM Training & Optimization (P2-010)
# =============================================================================
# Unsloth for fast LLM finetuning: 2x faster training with 80% less memory
# BUILDKIT_STARTER_SPEC.md Layer 12: LLMOps & Evaluation
# Activate with: pixi run -e finetuning <command>
#
# GPU REQUIREMENTS:
# - NVIDIA GPU with CUDA support (CUDA 11.8+ or 12.1+)
# - Minimum 8GB VRAM recommended (varies by model size)
# - For production: 16GB+ VRAM (e.g., RTX 4090, A100, H100)
# - CPU-only mode available but defeats the performance benefits
#
# CONDA-FORGE AVAILABILITY:
# - Unsloth is NOT available on conda-forge
# - Must be installed via PyPI (pip)
# - Dependencies (PyTorch, transformers, accelerate) are in base environment
#
# USAGE:
# - Combine with CUDA feature for GPU acceleration:
#   pixi run -e finetuning-cuda python train.py
# - Or use default (CPU-only, not recommended):
#   pixi run -e finetuning python train.py

[feature.finetuning]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.finetuning.dependencies]
# Core dependencies for finetuning (already in base, but explicit for clarity)
pytorch = ">=2.5,<3"
transformers = ">=4.47,<5"
accelerate = ">=1.2,<2"
datasets = ">=3.2,<4"
tokenizers = ">=0.21,<1"

# Training utilities
tqdm = ">=4.66"
peft = ">=0.8"        # Parameter-Efficient Fine-Tuning
bitsandbytes = ">=0.44"  # 8-bit optimizers

[feature.finetuning.pypi-dependencies]
# Unsloth - Fast LLM finetuning (PyPI-only, not on conda-forge)
# See: https://github.com/unslothai/unsloth
unsloth = ">=2024.0"

# =============================================================================
# Caching Feature - LLM Prompt & Response Caching (P3-001, P3-002)
# =============================================================================
# Semantic caching solutions for AI/LLM workloads to reduce latency and costs
# BUILDKIT_STARTER_SPEC.md Layer 10: State & Storage
# Activate with: pixi run -e caching <command>
#
# P3-001: vCache - Semantic prompt caching with guaranteed error bounds
# P3-002: prompt-cache - LLM proxy for semantic caching (Go binary in flake.nix)

[feature.caching]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.caching.dependencies]
# Core dependencies for caching operations
python = ">=3.11,<3.13"
redis-py = ">=4.5.1"         # Redis client for cache backend

# OpenAI for embedding and inference (vCache default)
openai = ">=1.0"

# Vector database support
numpy = ">=1.24,<3"

[feature.caching.pypi-dependencies]
# P3-001: vCache - Semantic Prompt Cache with Error Bounds
# See: https://github.com/vcache-project/vCache
# Description: First semantic prompt cache that guarantees user-defined error rate bounds
# Features:
#   - Reduces LLM latency by up to 100x and costs by up to 10x
#   - Online-learned, embedding-specific decision boundaries
#   - Supports FIFO, LRU, MRU, and custom SCU eviction policies
#   - Modular: configurable inference engines, embeddings, vector DBs
# Installation: pip install -e . (from cloned repo - not yet on PyPI)
# Usage:
#   from vcache import VCache, VerifiedDecisionPolicy
#   policy = VerifiedDecisionPolicy(delta=0.01)
#   vcache = VCache(policy=policy)
#   response = vcache.infer("Is the sky blue?")
# NOTE: Requires OpenAI API key via environment variable
# NOTE: Not yet available on PyPI - install from source:
#   git clone https://github.com/vcache-project/vCache.git
#   cd vCache && pip install -e .

# =============================================================================
# Vector Database Feature - ChromaDB (P1-001 Alternative)
# =============================================================================
# ChromaDB as alternative vector database for A/B testing with ruvector
# BUILDKIT_STARTER_SPEC.md Layer 10: State & Storage
# Activate with: pixi run -e vectordb-chromadb <command>
#
# Feature Flag: VECTOR_STORE=chromadb (in .env.state.example)
# Primary Use Case: Development, testing, embedded deployments
# Alternative: ruvector (Rust-native, via Cargo/npm - not Python)
#
# See: https://github.com/chroma-core/chroma

[feature.vectordb-chromadb]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.vectordb-chromadb.dependencies]
# ChromaDB - Embedding database for AI applications
chromadb = ">=0.4"

# Embedding model support
sentence-transformers = ">=2.0"

# HTTP client for remote ChromaDB
httpx = ">=0.24"

# Async support

# =============================================================================
# Vector Database Feature - RuVector Support (P1-001 Primary)
# =============================================================================
# RuVector is a Rust-native distributed vector database with GNN self-learning.
# BUILDKIT_STARTER_SPEC.md Layer 10: State & Storage
# Activate with: pixi run -e vectordb-ruvector <command>
#
# IMPORTANT: RuVector is NOT a Python package. It provides:
#   - Rust crates: cargo add ruvector-core ruvector-graph ruvector-gnn
#   - Node.js bindings: npm install ruvector
#   - WebAssembly: npm install ruvector-wasm
#   - CLI: cargo install ruvector-cli
#
# This feature provides Python client dependencies for HTTP/gRPC access.
# The actual ruvector server runs as a Rust binary or Docker container.
#
# Feature Flag: VECTOR_STORE=ruvector (in .env.state.example)
# Primary Use Case: Production, distributed deployments, GNN-enhanced search
#
# See: https://github.com/ruvnet/ruvector

[feature.vectordb-ruvector]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.vectordb-ruvector.dependencies]
# HTTP/gRPC clients for ruvector server communication
httpx = ">=0.24"
grpcio = ">=1.50"
grpcio-tools = ">=1.50"

# Protobuf for gRPC schemas
protobuf = ">=4.0"

# Embedding model support (for generating vectors to send to ruvector)
sentence-transformers = ">=2.0"

# Async support

# NumPy for vector operations
numpy = ">=1.24,<3"

[feature.vectordb-ruvector.pypi-dependencies]
# ruvector Python client (HTTP wrapper)
# Note: Full ruvector runs as Rust server; this is client-only
# Install server via: cargo install ruvector-server
# Or Docker: docker run -p 8000:8000 ruvector/ruvector

# =============================================================================
# QuDAG Feature - Quantum-Resistant Distributed Communication (P3-011)
# =============================================================================
# QuDAG is a quantum-resistant distributed communication platform for
# autonomous AI agents, swarm intelligence, and zero-person businesses.
# Activate with: pixi run -e qudag <command>
#
# IMPORTANT: QuDAG is NOT a Python package. It provides:
#   - Rust crates: cargo add qudag-core qudag-crypto qudag-network qudag-mcp
#   - Node.js bindings: npm install qudag
#   - CLI: cargo install qudag-cli
#   - WebAssembly: npm install qudag-wasm
#
# This feature provides Python client dependencies for QuDAG MCP server access.
#
# Key Features:
#   - Post-quantum cryptography (ML-KEM-768, ML-DSA, BLAKE3, HQC)
#   - DAG-based consensus (QR-Avalanche)
#   - Multi-hop onion routing with ChaCha20Poly1305
#   - Native MCP server (stdio, HTTP, WebSocket)
#   - AI agent coordination for autonomous systems
#
# See: https://github.com/ruvnet/qudag

[feature.qudag]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.qudag.dependencies]
# WebSocket client for QuDAG MCP connections
websockets = ">=11.0"

# HTTP client for QuDAG REST API
httpx = ">=0.24"

# Cryptography support (for verifying QuDAG signatures)
cryptography = ">=41.0"

# Async support

# JSON handling for MCP protocol
orjson = ">=3.9"

# Protobuf for binary protocol support
protobuf = ">=4.0"

[feature.qudag.pypi-dependencies]
# MCP Python SDK for QuDAG MCP server integration
# See: https://github.com/anthropics/anthropic-sdk-python
mcp = ">=1.0"

# =============================================================================
# Documentation Feature - MkDocs with Material Theme
# =============================================================================
# Build and serve documentation locally with MkDocs Material theme
# Activate with: pixi run -e docs mkdocs serve
#
# Features:
#   - Material theme with dark mode support
#   - Built-in search functionality
#   - Mermaid diagram rendering
#   - Code syntax highlighting
#   - Responsive navigation

[feature.docs]
platforms = ["linux-64", "osx-64", "osx-arm64", "linux-aarch64"]

[feature.docs.dependencies]
# MkDocs core
mkdocs = ">=1.6"

# Material theme
mkdocs-material = ">=9.5"
mkdocs-material-extensions = ">=1.3"

# Plugins

# PyMdown extensions
pymdown-extensions = ">=10.7"

# Image processing
pillow = ">=10.0"
cairosvg = ">=2.7"

[feature.docs.pypi-dependencies]
mkdocs-minify-plugin = ">=0.8"
[feature.docs.tasks]
# Serve documentation locally
docs-serve = "mkdocs serve"
# Build documentation
docs-build = "mkdocs build --strict"

[environments]
default = { features = ["ros"], solve-group = "default" }
cuda = { features = ["cuda"], solve-group = "cuda" }
aios = { features = ["aios"], solve-group = "aios" }
aios-cuda = { features = ["aios", "aios-cuda", "cuda"], solve-group = "aios-cuda" }
llmops = { features = ["llmops"], solve-group = "llmops" }
finetuning = { features = ["finetuning"], solve-group = "finetuning" }
finetuning-cuda = { features = ["finetuning", "cuda"], solve-group = "finetuning-cuda" }
caching = { features = ["caching"], solve-group = "caching" }
docs = { features = ["docs"], solve-group = "docs" }
# Vector database environments for A/B testing
vectordb-chromadb = { features = ["vectordb-chromadb"], solve-group = "vectordb-chromadb" }
vectordb-ruvector = { features = ["vectordb-ruvector"], solve-group = "vectordb-ruvector" }
# QuDAG quantum-resistant distributed communication
qudag = { features = ["qudag"], solve-group = "qudag" }

[feature.llmops.pypi-dependencies]
trulens-eval = ">=0.30"
