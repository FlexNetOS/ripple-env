# LocalAI - OpenAI-Compatible Inference Service (BUILDKIT L9)
# P1-001: LocalAI is the primary inference plane for the agentic OS
#
# LocalAI provides:
#   - OpenAI-compatible API (drop-in replacement)
#   - Support for multiple model backends (llama.cpp, vLLM, etc.)
#   - Model management and loading
#   - GPU acceleration support
#   - Distributed inference capabilities
#
# Features:
#   - REST API compatible with OpenAI clients
#   - gRPC support for high-performance scenarios
#   - WebUI for model management
#   - Automatic model downloading
#   - Multi-GPU support
#
# References:
#   - https://localai.io
#   - https://github.com/mudler/LocalAI
#   - BUILDKIT_STARTER_SPEC.md L9: Inference Plane

services:
  localai:
    image: localai/localai:latest-aio-cpu
    container_name: localai
    restart: unless-stopped
    
    # Environment variables
    environment:
      # Model configuration
      MODELS_PATH: /models
      THREADS: 4  # Adjust based on CPU cores
      
      # API configuration
      ADDRESS: 0.0.0.0:8080
      
      # Context size
      CONTEXT_SIZE: 2048
      
      # Batch size for processing
      BATCH_SIZE: 512
      
      # Enable GPU (if available)
      # GPU_LAYERS: 20  # Number of layers to offload to GPU
      
      # Model settings
      DEFAULT_MODEL: llama-3.2-3b-instruct
      
      # CORS settings
      CORS_ORIGINS: "*"
      CORS_ALLOW_CREDENTIALS: "true"
      
      # Logging
      DEBUG: "false"
      
      # Parallel requests
      PARALLEL_REQUESTS: "true"
      
      # Enable embeddings
      EMBEDDINGS: "true"
    
    # Ports
    ports:
      - "8080:8080"  # LocalAI API
      - "8090:8090"  # WebUI (if available)
    
    # Volumes for models and data
    volumes:
      # Models directory (pre-downloaded models)
      - ./data/localai/models:/models
      # Configuration
      - ./config/localai:/config
      # Temporary files
      - ./tmp/localai:/tmp/localai
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '1.0'
          memory: 2G
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Security settings
    security_opt:
      - no-new-privileges:true
    
    # Network
    networks:
      - localai-network
    
    # Labels for documentation and management
    labels:
      - "service=localai"
      - "layer=L9-Inference-Plane"
      - "description=LocalAI inference service"
      - "traefik.enable=true"
      - "traefik.http.routers.localai.rule=Host(`ai.localhost`)"
      - "traefik.http.routers.localai.entrypoints=web"
      - "traefik.http.services.localai.loadbalancer.server.port=8080"

  # Redis for caching (optional but recommended)
  redis:
    image: redis:7-alpine
    container_name: localai-redis
    restart: unless-stopped
    
    # Redis configuration
    command: >
      redis-server 
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
    
    # Ports
    ports:
      - "6379:6379"
    
    # Volumes
    volumes:
      - localai-redis-data:/data
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
    
    # Network
    networks:
      - localai-network

# Volumes
volumes:
  localai-redis-data:
    driver: local

# Networks
networks:
  localai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16