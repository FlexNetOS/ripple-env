# Lightweight AI Stack for CPU-Only Development
#
# Minimal resource configuration for development and testing.
# Use this profile when:
# - Running on systems with limited RAM (4-8GB)
# - No GPU available
# - Quick iteration and testing
# - CI/CD environments
#
# Requirements:
# - RAM: 4GB minimum, 8GB recommended
# - CPU: 2-4 cores
# - Disk: 10GB free
#
# Usage:
#   docker compose -f docker/docker-compose.lightweight.yml up -d
#
# Validate resources first:
#   ./scripts/validate-resources.sh --profile lightweight

name: flexstack-lightweight

services:
  # LocalAI - Lightweight CPU configuration
  localai:
    image: localai/localai:v2.24.2-aio-cpu
    container_name: localai-lightweight
    ports:
      - "8080:8080"
    environment:
      # Minimal threading for low-resource systems
      - THREADS=2
      # Reduced context for faster inference
      - CONTEXT_SIZE=512
      # Single request at a time
      - PARALLEL=1
      # Enable FP16 for efficiency
      - F16=true
      # Model path
      - MODELS_PATH=/models
      # Reduce debug output
      - DEBUG=false
      # Memory-mapped model loading (reduces RAM usage)
      - LOCALAI_MMAP=true
      # Disable backends we don't need
      - DISABLE_AUTOLOAD_MODELS=true
    volumes:
      - ./data/localai/models:/models:cached
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    networks:
      - lightweight-network

  # Redis - Minimal cache for basic caching
  redis:
    image: redis:7-alpine
    container_name: redis-lightweight
    ports:
      - "6379:6379"
    command: >
      redis-server
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --save ""
      --appendonly no
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
    networks:
      - lightweight-network

networks:
  lightweight-network:
    driver: bridge

# Volume configuration
# Note: Uses bind mounts instead of named volumes for easier cleanup
# Volumes are created in ./data/ relative to docker-compose file location

# Model recommendations for lightweight profile:
#
# Small models (< 2GB each, good for 4GB RAM):
#   - Qwen3-0.6B-BF16.gguf (~1.2GB) - Ultra-fast, basic tasks
#   - gemma-3-1b-it-BF16.gguf (~2GB) - Good quality, general purpose
#   - DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf (~1.5GB) - Code assistance
#
# Download recommended lightweight models:
#   ./scripts/download-models.sh --lightweight
#
# Expected performance (on 4-core CPU):
#   - Inference: 5-15 tokens/second
#   - First token latency: 500-1000ms
#   - Context: 512 tokens (sufficient for short conversations)
